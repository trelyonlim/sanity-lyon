'use strict';

var _slicedToArray = function () { function sliceIterator(arr, i) { var _arr = []; var _n = true; var _d = false; var _e = undefined; try { for (var _i = arr[Symbol.iterator](), _s; !(_n = (_s = _i.next()).done); _n = true) { _arr.push(_s.value); if (i && _arr.length === i) break; } } catch (err) { _d = true; _e = err; } finally { try { if (!_n && _i["return"]) _i["return"](); } finally { if (_d) throw _e; } } return _arr; } return function (arr, i) { if (Array.isArray(arr)) { return arr; } else if (Symbol.iterator in Object(arr)) { return sliceIterator(arr, i); } else { throw new TypeError("Invalid attempt to destructure non-iterable instance"); } }; }();

let uploadAssets = (() => {
  var _ref = _asyncToGenerator(function* (assets, options) {
    // Build a Map where the keys are `type#url` and the value is an array of all
    // objects containing document id and path to inject asset reference to.
    // `assets` is an array of objects with shape: {documentId, path, url, type}
    const assetRefMap = getAssetRefMap(assets)

    // We might have additional assets that is not referenced by any documents, but was part of a
    // dataset when exporting, for instance. Add these to the map without any references to update.
    ;(options.unreferencedAssets || []).forEach(function (asset) {
      if (!assetRefMap.has(asset)) {
        assetRefMap.set(asset, []);
      }
    });

    // Create a function we can call for every completed upload to report progress
    const progress = progressStepper(options.onProgress, {
      step: 'Importing assets (files/images)',
      total: assetRefMap.size
    });

    // Loop over all unique URLs and ensure they exist, and if not, upload them
    const mapOptions = { concurrency: ASSET_UPLOAD_CONCURRENCY };
    const assetIds = yield pMap(assetRefMap.keys(), ensureAsset.bind(null, options, progress), mapOptions);

    // Loop over all documents that need asset references to be set
    const batches = yield setAssetReferences(assetRefMap, assetIds, options);
    return batches.reduce(function (prev, add) {
      return prev + add;
    }, 0);
  });

  return function uploadAssets(_x, _x2) {
    return _ref.apply(this, arguments);
  };
})();

let ensureAsset = (() => {
  var _ref2 = _asyncToGenerator(function* (options, progress, assetKey, i) {
    const client = options.client;
    var _options$assetMap = options.assetMap;
    const assetMap = _options$assetMap === undefined ? {} : _options$assetMap;

    var _assetKey$split = assetKey.split('#', 2),
        _assetKey$split2 = _slicedToArray(_assetKey$split, 2);

    const type = _assetKey$split2[0],
          url = _assetKey$split2[1];

    // Download the asset in order for us to create a hash

    debug('[Asset #%d] Downloading %s', i, url);

    var _ref3 = yield getHashedBufferForUri(url);

    const buffer = _ref3.buffer,
          sha1hash = _ref3.sha1hash;

    // See if the item exists on the server

    debug('[Asset #%d] Checking for asset with hash %s', i, sha1hash);
    const assetDocId = yield getAssetDocumentIdForHash(client, type, sha1hash);
    if (assetDocId) {
      // Same hash means we want to reuse the asset
      debug('[Asset #%d] Found %s for hash %s', i, type, sha1hash);
      progress();
      return assetDocId;
    }

    const assetMeta = assetMap[`${type}-${sha1hash}`];
    const hasFilename = assetMeta && assetMeta.originalFilename;
    const hasNonFilenameMeta = assetMeta && Object.keys(assetMap).length > 1;

    var _parseUrl = parseUrl(url);

    const pathname = _parseUrl.pathname;

    const filename = hasFilename ? assetMeta.originalFilename : basename(pathname);

    // If it doesn't exist, we want to upload it
    debug('[Asset #%d] Uploading %s with URL %s', i, type, url);
    const asset = yield client.assets.upload(type, buffer, { filename });
    progress();

    // If we have more metadata to provide, update the asset document
    if (hasNonFilenameMeta) {
      yield client.patch(asset._id).set(assetMeta);
    }

    return asset._id;
  });

  return function ensureAsset(_x3, _x4, _x5, _x6) {
    return _ref2.apply(this, arguments);
  };
})();

let getAssetDocumentIdForHash = (() => {
  var _ref4 = _asyncToGenerator(function* (client, type, sha1hash, attemptNum = 0) {
    // @todo remove retry logic when client has reintroduced it
    try {
      const dataType = type === 'file' ? 'sanity.fileAsset' : 'sanity.imageAsset';
      const query = '*[_type == $dataType && sha1hash == $sha1hash][0]._id';
      const assetDocId = yield client.fetch(query, { dataType, sha1hash });
      return assetDocId;
    } catch (err) {
      if (attemptNum < 3) {
        return getAssetDocumentIdForHash(client, type, sha1hash, attemptNum + 1);
      }

      err.attempts = attemptNum;
      throw new Error(`Error while attempt to query Sanity API:\n${err.message}`);
    }
  });

  return function getAssetDocumentIdForHash(_x7, _x8, _x9) {
    return _ref4.apply(this, arguments);
  };
})();

function _asyncToGenerator(fn) { return function () { var gen = fn.apply(this, arguments); return new Promise(function (resolve, reject) { function step(key, arg) { try { var info = gen[key](arg); var value = info.value; } catch (error) { reject(error); return; } if (info.done) { resolve(value); } else { return Promise.resolve(value).then(function (value) { step("next", value); }, function (err) { step("throw", err); }); } } return step("next"); }); }; }

const basename = require('path').basename;
const parseUrl = require('url').parse;
const debug = require('debug')('sanity:import');
const pMap = require('p-map');
const progressStepper = require('./util/progressStepper');
const getHashedBufferForUri = require('./util/getHashedBufferForUri');

const ASSET_UPLOAD_CONCURRENCY = 3;
const ASSET_PATCH_CONCURRENCY = 3;
const ASSET_PATCH_BATCH_SIZE = 50;

function getAssetRefMap(assets) {
  return assets.reduce((assetRefMap, item) => {
    const documentId = item.documentId,
          path = item.path,
          url = item.url,
          type = item.type;

    const key = `${type}#${url}`;
    let refs = assetRefMap.get(key);
    if (!refs) {
      refs = [];
      assetRefMap.set(key, refs);
    }

    refs.push({ documentId, path });
    return assetRefMap;
  }, new Map());
}

function setAssetReferences(assetRefMap, assetIds, options) {
  const client = options.client;

  const lookup = assetRefMap.values();
  const patchTasks = assetIds.reduce((tasks, assetId) => {
    const documents = lookup.next().value;
    return tasks.concat(documents.map(({ documentId, path }) => ({
      documentId,
      path,
      assetId
    })));
  }, []);

  // We now have an array of simple tasks, each containing:
  // {documentId, path, assetId}
  // Instead of doing a single mutation per asset, let's batch them up
  const batches = [];
  for (let i = 0; i < patchTasks.length; i += ASSET_PATCH_BATCH_SIZE) {
    batches.push(patchTasks.slice(i, i + ASSET_PATCH_BATCH_SIZE));
  }

  // Since separate progress step for batches of reference sets
  const progress = progressStepper(options.onProgress, {
    step: 'Setting asset references to documents',
    total: batches.length
  });

  // Now perform the batch operations in parallel with a given concurrency
  const mapOptions = { concurrency: ASSET_PATCH_CONCURRENCY };
  return pMap(batches, setAssetReferenceBatch.bind(null, client, progress), mapOptions);
}

function setAssetReferenceBatch(client, progress, batch) {
  debug('Setting asset references on %d documents', batch.length);
  return batch.reduce(reducePatch, client.transaction()).commit({ visibility: 'async' }).then(progress).then(res => res.results.length);
}

function getAssetType(assetId) {
  return assetId.slice(0, assetId.indexOf('-'));
}

function reducePatch(trx, task) {
  return trx.patch(task.documentId, patch => patch.set({
    [`${task.path}._type`]: getAssetType(task.assetId),
    [`${task.path}.asset`]: {
      _type: 'reference',
      _ref: task.assetId
    }
  }));
}

module.exports = uploadAssets;