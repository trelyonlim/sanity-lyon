'use strict';

var _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; };

function _asyncToGenerator(fn) { return function () { var gen = fn.apply(this, arguments); return new Promise(function (resolve, reject) { function step(key, arg) { try { var info = gen[key](arg); var value = info.value; } catch (error) { reject(error); return; } if (info.done) { resolve(value); } else { return Promise.resolve(value).then(function (value) { step("next", value); }, function (err) { step("throw", err); }); } } return step("next"); }); }; }

const os = require('os');
const fs = require('fs');
const path = require('path');
const miss = require('mississippi');
const gunzipMaybe = require('gunzip-maybe');
const peek = require('peek-stream');
const isTar = require('is-tar');
const tar = require('tar-fs');
const globby = require('globby');
const debug = require('debug')('sanity:import:stream');

var _require = require('lodash');

const noop = _require.noop;

const getJsonStreamer = require('./util/getJsonStreamer');

module.exports = (stream, options, importers) => new Promise((resolve, reject) => {
  let findAndImport = (() => {
    var _ref = _asyncToGenerator(function* () {
      debug('Tarball extracted, looking for ndjson');

      const files = yield globby('**/*.ndjson', { cwd: outputPath, deep: 2, absolute: true });
      if (!files.length) {
        reject(new Error('ndjson-file not found in tarball'));
        return;
      }

      const importBaseDir = path.dirname(files[0]);
      resolve(importers.fromFolder(importBaseDir, _extends({}, options, { deleteOnComplete: true }), importers));
    });

    return function findAndImport() {
      return _ref.apply(this, arguments);
    };
  })();

  const outputPath = path.join(os.tmpdir(), 'sanity-import');
  debug('Importing from stream');

  let isTarStream = false;
  let jsonDocuments;

  const uncompressStream = miss.pipeline(gunzipMaybe(), untarMaybe());
  miss.pipe(stream, uncompressStream, err => {
    if (err) {
      reject(err);
      return;
    }

    if (isTarStream) {
      findAndImport();
    } else {
      resolve(importers.fromArray(jsonDocuments, options));
    }
  });

  function untarMaybe() {
    return peek({ newline: false, maxBuffer: 300 }, (data, swap) => {
      if (isTar(data)) {
        debug('Stream is a tarball, extracting to %s', outputPath);
        isTarStream = true;
        return swap(null, tar.extract(outputPath));
      }

      debug('Stream is an ndjson file, streaming JSON');
      const jsonStreamer = getJsonStreamer();
      const concatter = miss.concat(resolveNdjsonStream);
      const ndjsonStream = miss.pipeline(jsonStreamer, concatter);
      ndjsonStream.on('error', err => {
        uncompressStream.emit('error', err);
        destroy([uncompressStream, jsonStreamer, concatter, ndjsonStream]);
        reject(err);
      });
      return swap(null, ndjsonStream);
    });
  }

  function resolveNdjsonStream(documents) {
    debug('Finished reading ndjson stream');
    jsonDocuments = documents;
  }
});

function destroy(streams) {
  streams.forEach(stream => {
    if (isFS(stream)) {
      // use close for fs streams to avoid fd leaks
      stream.close(noop);
    } else if (isRequest(stream)) {
      // request.destroy just do .end - .abort is what we want
      stream.abort();
    } else if (isFn(stream.destroy)) {
      stream.destroy();
    }
  });
}

function isFn(fn) {
  return typeof fn === 'function';
}

function isFS(stream) {
  return (stream instanceof (fs.ReadStream || noop) || stream instanceof (fs.WriteStream || noop)) && isFn(stream.close);
}

function isRequest(stream) {
  return stream.setHeader && isFn(stream.abort);
}